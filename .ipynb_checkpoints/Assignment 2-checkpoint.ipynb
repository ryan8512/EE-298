{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters, Imports, and Inputs\n",
    "\n",
    "#### Declare Important Libraries, Parameters for Dataset, and Inputs of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "histogram_bins = 1000\n",
    "samples = 1000000\n",
    "training_samples = int(0.9*samples)\n",
    "testing_samples = int(0.1*samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input the mean of the gaussian curve:0\n",
      "Please input the stardard deviation of the gaussian curve:1\n"
     ]
    }
   ],
   "source": [
    "mean = int(input(\"Please input the mean of the gaussian curve:\"))\n",
    "std_dev = int(input(\"Please input the stardard deviation of the gaussian curve:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset and Histogramming\n",
    "<ol>\n",
    "<li> Draw 1,000,00 samples and store it in raw_dataset variable \n",
    "<li> Take the minimum and maximum and use linspace to know the starting values of the \"bins\"\n",
    "<li> Declare mid_points to take the average of each histogram bins (not the starting points)\n",
    "<li> Declare count to count how much values are in each \"bins\"\n",
    "<li> Start a nested loop that has the histogram bins as the outerloop and random dataset as the inner loop. Take note that this is an inefficient solution since it would loop 1 billion times! See time elapse \n",
    "<li> Count the number of occurence and also get the midpoint of each histogram bins\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time took: 231.12993621826172\n",
      "Total number of samples check:1000000.0\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = np.random.normal(mean, std_dev,samples)\n",
    "starting_points = np.linspace(np.min(raw_dataset), np.max(raw_dataset), num=histogram_bins, endpoint=False)\n",
    "mid_points = np.zeros(histogram_bins)\n",
    "count = np.zeros(histogram_bins)\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(histogram_bins):\n",
    "    #Edge cases\n",
    "    if i == histogram_bins-1:\n",
    "        maximum = np.max(raw_dataset)\n",
    "    else:\n",
    "        maximum = starting_points[i+1]\n",
    "    \n",
    "    #Get the midpoints\n",
    "    mid_points[i] = starting_points[i]+(starting_points[i] - maximum)/2\n",
    "    \n",
    "    #Sweep through the dataset\n",
    "    for j in raw_dataset:\n",
    "        if ((j >= starting_points[i]) and (j <= maximum)): #Assume no EXACTLY equal number\n",
    "            count[i] = count[i] + 1\n",
    "    \n",
    "end_time = time.time() - start_time\n",
    "print(\"Time took: \"+str(end_time))\n",
    "print(\"Total number of samples check:\"+str(np.sum(count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Translate the midpoints and the count to a 1,000,000 random dataset with the midpoints as the value of each histograms.\n",
    "\n",
    "#### Note that they are only 1,000 values (maximum) as the bins with some other bins near the average having a larger count, hence more number in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of testing dataset (histogrammed):(900000,)\n",
      "Size of testing dataset (histogrammed):(100000,)\n"
     ]
    }
   ],
   "source": [
    "def translate_count(count, mid_points):\n",
    "    output = []\n",
    "    for index, occurrence in enumerate(count):\n",
    "        if(occurrence !=0):\n",
    "            for _ in range(int(occurrence)):\n",
    "                output.append(mid_points[index])\n",
    "    output_array = np.array(output)\n",
    "    return output_array\n",
    "\n",
    "dataset = translate_count(count,mid_points)\n",
    "random_dataset = np.random.choice(dataset,samples,replace=False)\n",
    "training_dataset = random_dataset[0:training_samples]\n",
    "testing_dataset = random_dataset[training_samples:(training_samples + testing_samples)]\n",
    "\n",
    "print(\"Size of testing dataset (histogrammed):\"+str(training_dataset.shape))\n",
    "print(\"Size of testing dataset (histogrammed):\"+str(testing_dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 3. Build a 3 layer neural network </h1>\n",
    "    \n",
    "Strong inspiration: https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 1, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 1, \"activation\": \"relu\"},\n",
    "]\n",
    "\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    #np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        params_values['W' + str(layer_idx)] = np.random.normal(0,0.1,(layer_output_size, layer_input_size))\n",
    "        params_values['b' + str(layer_idx)] = np.random.normal(0,0.1,(layer_output_size, 1))\n",
    "        \n",
    "    return params_values\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "    \n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;\n",
    "    \n",
    "\n",
    "\n",
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        activation_func = relu\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        \n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    return A_curr, memory\n",
    "\n",
    "def get_cost_value(predictions,targets):\n",
    "    # Retrieving number of samples in dataset\n",
    "    samples_num = len(predictions)\n",
    "    \n",
    "    # Summing square differences between predicted and expected values\n",
    "    accumulated_error = 0.0\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        accumulated_error += (prediction - target)**2\n",
    "        \n",
    "    # Calculating mean and dividing by 2\n",
    "    mae_error = (1.0 / (2*samples_num)) * accumulated_error\n",
    "    \n",
    "    return mae_error\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    return 100 - np.mean(np.abs(Y_hat - Y)) * 100\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    dA_prev = (Y_hat - Y)\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "        \n",
    "    \n",
    "    return grads_values\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    for layer_idx, layer in enumerate(nn_architecture):\n",
    "        params_values[\"W\" + str(layer_idx+1)] -= learning_rate * grads_values[\"dW\" + str(layer_idx+1)]        \n",
    "        params_values[\"b\" + str(layer_idx+1)] -= learning_rate * grads_values[\"db\" + str(layer_idx+1)]\n",
    "\n",
    "    return params_values;\n",
    "\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    params_values = init_layers(nn_architecture, 1)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for j in range(X.shape[1]):\n",
    "            Y_hat, cashe = full_forward_propagation(np.array([X[0][j]])[np.newaxis,:], params_values, nn_architecture)\n",
    "            cost = get_cost_value(Y_hat, np.array([Y[0][j]])[np.newaxis,:])\n",
    "            cost_history.append(cost)\n",
    "            accuracy = get_accuracy_value(Y_hat, np.array([Y[0][j]])[np.newaxis,:])\n",
    "            accuracy_history.append(accuracy)\n",
    "        \n",
    "            grads_values = full_backward_propagation(Y_hat, np.array([Y[0][j]])[np.newaxis,:], cashe, params_values, nn_architecture)\n",
    "        \n",
    "            params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "    return params_values, cost_history, accuracy_history \n",
    "        \n",
    "def test(X, Y, nn_architecture,params_values):\n",
    "    Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "    cost = get_cost_value(Y_hat, Y)\n",
    "    accuracy = get_accuracy_value(Y_hat, Y)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def normal(x,mu,sigma):\n",
    "    return ( 2.*np.pi*sigma**2. )**-.5 * np.exp( -.5 * (x-mu)**2. / sigma**2. )\n",
    "\n",
    "def real_function(inputs,mean,std_dev):\n",
    "    output = []\n",
    "    for i in range(inputs.shape[0]):\n",
    "        gaussian_eq = normal(inputs[i],mean,std_dev)\n",
    "        output.append(relu(gaussian_eq))\n",
    "    \n",
    "    return np.array(output)[np.newaxis,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = real_function(training_dataset,mean,std_dev)\n",
    "params_values, cost_history, accuracy_history = train(training_dataset[np.newaxis,:],y,nn_architecture,1,0.1)\n",
    "print(accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.80346654591341\n",
      "71.80346654591341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.48787931]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = real_function(testing_dataset,mean,std_dev)\n",
    "print(test(testing_dataset[np.newaxis,:],z,nn_architecture,params_values))\n",
    "print(test(testing_dataset[np.newaxis,:],z,nn_architecture,params_values))\n",
    "\n",
    "hi = training_dataset[np.newaxis,:]\n",
    "\n",
    "np.array([hi[0][1]])[np.newaxis,:]\n",
    "\n",
    "#np.array([training_dataset[0]])[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
