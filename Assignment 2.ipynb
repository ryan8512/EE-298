{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1. Parameters, Imports, and Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "histogram_bins = 1000\n",
    "samples = 1000000\n",
    "training_samples = int(0.9*samples)\n",
    "testing_samples = int(0.1*samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0 #input(\"Please input the mean of the gaussian curve:\")\n",
    "std_dev = 1 #input(\"Please input the stardard deviation of the gaussian curve:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2. Dataset and Histogramming </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time took: 241.71027326583862\n",
      "Total number of samples check:1000000.0\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = np.random.normal(mean, std_dev,samples)\n",
    "starting_points = np.linspace(np.min(raw_dataset), np.max(raw_dataset), num=histogram_bins, endpoint=False)\n",
    "mid_points = np.zeros(histogram_bins)\n",
    "count = np.zeros(histogram_bins)\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(histogram_bins):\n",
    "    #Edge cases\n",
    "    if i == histogram_bins-1:\n",
    "        maximum = np.max(raw_dataset)\n",
    "    else:\n",
    "        maximum = starting_points[i+1]\n",
    "    \n",
    "    #Get the midpoints\n",
    "    mid_points[i] = starting_points[i]+(starting_points[i] - maximum)/2\n",
    "    \n",
    "    #Sweep through the dataset\n",
    "    for j in raw_dataset:\n",
    "        if ((j >= starting_points[i]) and (j <= maximum)): #Assume no EXACTLY equal number\n",
    "            count[i] = count[i] + 1\n",
    "    \n",
    "end_time = time.time() - start_time\n",
    "print(\"Time took: \"+str(end_time))\n",
    "print(\"Total number of samples check:\"+str(np.sum(count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of testing dataset (histogrammed):(900000,)\n",
      "Size of testing dataset (histogrammed):(100000,)\n"
     ]
    }
   ],
   "source": [
    "def translate_count(count, mid_points):\n",
    "    output = []\n",
    "    for index, occurrence in enumerate(count):\n",
    "        if(occurrence !=0):\n",
    "            for _ in range(int(occurrence)):\n",
    "                output.append(mid_points[index])\n",
    "    output_array = np.array(output)\n",
    "    return output_array\n",
    "\n",
    "dataset = translate_count(count,mid_points)\n",
    "random_dataset = np.random.choice(dataset,samples,replace=False)\n",
    "training_dataset = random_dataset[0:training_samples]\n",
    "testing_dataset = random_dataset[training_samples:(training_samples + testing_samples)]\n",
    "\n",
    "print(\"Size of testing dataset (histogrammed):\"+str(training_dataset.shape))\n",
    "print(\"Size of testing dataset (histogrammed):\"+str(testing_dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 3. Build a 3 layer neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 1, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 1, \"activation\": \"relu\"},\n",
    "]\n",
    "\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        params_values['W' + str(layer_idx)] = np.random.normal(0,0.1,(layer_output_size, layer_input_size))\n",
    "        params_values['b' + str(layer_idx)] = np.random.normal(0,0.1,(layer_output_size, 1))\n",
    "        \n",
    "    return params_values\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;\n",
    "\n",
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        \n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    return A_curr, memory\n",
    "\n",
    "def get_cost_value(predictions,targets):\n",
    "    # Retrieving number of samples in dataset\n",
    "    samples_num = len(predictions)\n",
    "    \n",
    "    # Summing square differences between predicted and expected values\n",
    "    accumulated_error = 0.0\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        accumulated_error += (prediction - target)**2\n",
    "        \n",
    "    # Calculating mean and dividing by 2\n",
    "    mae_error = (1.0 / (2*samples_num)) * accumulated_error\n",
    "    \n",
    "    return mae_error\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    return 100 - np.mean(np.abs(Y_hat - Y)) * 100\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation == \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    dA_prev = (Y_hat - Y)\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "        \n",
    "    \n",
    "    return grads_values\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    for layer_idx, layer in enumerate(nn_architecture):\n",
    "        params_values[\"W\" + str(layer_idx+1)] -= learning_rate * grads_values[\"dW\" + str(layer_idx+1)]        \n",
    "        params_values[\"b\" + str(layer_idx+1)] -= learning_rate * grads_values[\"db\" + str(layer_idx+1)]\n",
    "\n",
    "    return params_values;\n",
    "\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        \n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "def test(X, Y, nn_architecture,params_values):\n",
    "    Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "    cost = get_cost_value(Y_hat, Y)\n",
    "    accuracy = get_accuracy_value(Y_hat, Y)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def normal(x,mu,sigma):\n",
    "    return ( 2.*np.pi*sigma**2. )**-.5 * np.exp( -.5 * (x-mu)**2. / sigma**2. )\n",
    "\n",
    "def real_function(inputs,mean,std_dev):\n",
    "    output = []\n",
    "    for i in range(inputs.shape[0]):\n",
    "        gaussian_eq = normal(inputs[i],mean,std_dev)\n",
    "        output.append(relu(gaussian_eq))\n",
    "    \n",
    "    return np.array(output)[np.newaxis,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75.12741103757014, 79.67488630287781, 82.83403678890062, 85.00554205898061, 86.50539734086693, 87.54935915595473, 88.27992433057585, 88.79595099401638, 89.16728983248213, 89.43949868382937, 89.64279470304155, 89.79838946834379, 89.91964767395626, 90.01711412967546, 90.0969989696036, 90.16389758548318, 90.22109342838961, 90.27178555829903, 90.31701113723148, 90.35843683431261]\n"
     ]
    }
   ],
   "source": [
    "y = real_function(training_dataset,mean,std_dev)\n",
    "params_values, cost_history, accuracy_history = train(training_dataset[np.newaxis,:],y,nn_architecture,20,0.1)\n",
    "print(accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.39437670370042\n",
      "90.39437670370042\n"
     ]
    }
   ],
   "source": [
    "z = real_function(testing_dataset,mean,std_dev)\n",
    "print(test(testing_dataset[np.newaxis,:],z,nn_architecture,params_values))\n",
    "print(test(testing_dataset[np.newaxis,:],z,nn_architecture,params_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
